#!/usr/bin/env python
"""
PDF UPLOAD SYSTEM - COMPREHENSIVE GUIDE
=========================================

This document explains how PDF document uploads work in GraphRAG
"""

# ============================================================================
# HOW PDF UPLOAD WORKS - STEP BY STEP
# ============================================================================

"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PDF UPLOAD WORKFLOW                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. USER INITIATES UPLOAD
   â”œâ”€ Run: python main.py
   â”œâ”€ Select: [2] Upload PDF/Document file
   â””â”€ Choose: [1] Single file or [2] Directory

2. FILE SELECTION
   â”œâ”€ Enter file path: /path/to/document.pdf
   â”œâ”€ System checks: File exists? Format supported?
   â””â”€ Supported formats: .pdf, .txt, .docx, .pptx

3. TEXT EXTRACTION
   â”œâ”€ PDF Files:
   â”‚  â”œâ”€ Use: PyPDF2.PdfReader
   â”‚  â”œâ”€ Extract: Text from all pages
   â”‚  â”œâ”€ Preserve: Page breaks and structure
   â”‚  â””â”€ Result: 5000+ characters typically
   â”‚
   â”œâ”€ TXT Files:
   â”‚  â”œâ”€ Use: file.read() with UTF-8 encoding
   â”‚  â”œâ”€ Extract: Raw text content
   â”‚  â””â”€ Result: Direct file content
   â”‚
   â”œâ”€ DOCX Files:
   â”‚  â”œâ”€ Use: python-docx library
   â”‚  â”œâ”€ Extract: Paragraphs, tables, formatting
   â”‚  â””â”€ Result: Structured text
   â”‚
   â””â”€ PPTX Files:
      â”œâ”€ Use: python-pptx library
      â”œâ”€ Extract: Slide text and notes
      â””â”€ Result: Slide content

4. TEXT CLEANING
   â”œâ”€ Remove multiple spaces â†’ single spaces
   â”œâ”€ Remove special characters
   â”œâ”€ Remove null bytes (\x00)
   â”œâ”€ Normalize whitespace
   â””â”€ Preserve readability

5. DOCUMENT INGESTION (Memory Manager)
   â”œâ”€ Chunk document: split_document()
   â”‚  â”œâ”€ Strategy: Sentence-based chunking
   â”‚  â”œâ”€ Size: 80-100 words per chunk
   â”‚  â”œâ”€ Overlap: 15 words between chunks
   â”‚  â”œâ”€ Detect: [NEG] markers for negations
   â”‚  â””â”€ Result: 3-50 chunks depending on size
   â”‚
   â”œâ”€ Create embeddings: OpenAI Embeddings
   â”‚  â”œâ”€ Model: text-embedding-3-small
   â”‚  â”œâ”€ Dimension: 1536
   â”‚  â””â”€ Cost: Minimal (cheap embedding model)
   â”‚
   â”œâ”€ Store in FAISS: Vector database
   â”‚  â”œâ”€ Location: faiss_index/ directory
   â”‚  â”œâ”€ Method: Flat index (no compression)
   â”‚  â”œâ”€ Speed: O(1) save/load, O(n) search
   â”‚  â””â”€ Persistence: Automatically saved
   â”‚
   â”œâ”€ Extract entities: Named Entity Recognition
   â”‚  â”œâ”€ From: LLM analysis of chunks
   â”‚  â”œâ”€ Types: Person, Organization, Location
   â”‚  â””â”€ Store: Neo4j graph database
   â”‚
   â””â”€ Create relationships:
      â”œâ”€ NEXT: Between consecutive chunks
      â”œâ”€ RELATED: Between similar chunks
      â””â”€ Store: Neo4j graph database

6. METADATA STORAGE
   â”œâ”€ For each chunk:
   â”‚  â”œâ”€ doc_id: company_info_1
   â”‚  â”œâ”€ source: company_info (user-provided)
   â”‚  â”œâ”€ timestamp: 2025-12-19T12:05:16
   â”‚  â”œâ”€ chunk_index: 0, 1, 2, ...
   â”‚  â””â”€ priority: 1.0 (default)
   â”‚
   â””â”€ In Neo4j and FAISS vector store

7. QUERY RETRIEVAL (When user asks question)
   â”œâ”€ Convert query to embedding: OpenAI
   â”œâ”€ Search FAISS: k=15 nearest chunks
   â”œâ”€ Re-rank: By entity matching
   â”œâ”€ Add Neo4j: Graph relationships
   â”œâ”€ Combine: All relevant context
   â”œâ”€ Send to LLM: GPT-4o-mini
   â””â”€ Generate: Answer with citations

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         COMPLETE DATA FLOW                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: company.pdf (250KB, 10 pages)
   â†“
EXTRACTION: 5000 characters
   â†“
CLEANING: Normalized text
   â†“
CHUNKING: 50 chunks (80-100 words each)
   â”œâ”€ Chunk 1: "Company overview... GraphRAG Corp..."
   â”œâ”€ Chunk 2: "Our team includes Parth Kumar as CTO..."
   â”œâ”€ Chunk 3: "Headquarters in Mumbai, India..."
   â””â”€ ...
   â†“
EMBEDDING: 50 vectors (1536-dim each)
   â”œâ”€ Vector 1: [0.123, 0.456, -0.789, ...]
   â”œâ”€ Vector 2: [0.234, -0.567, 0.890, ...]
   â””â”€ ...
   â†“
FAISS STORAGE:
   â”œâ”€ File: faiss_index/index.bin (binary vector index)
   â”œâ”€ File: faiss_index/metadata.json (chunk info)
   â””â”€ File: faiss_index/chunk_store.pkl (chunks data)
   â†“
NEO4J STORAGE:
   â”œâ”€ Nodes:
   â”‚  â”œâ”€ (:Chunk {id: "chunk_0", text: "...", source: "company_info"})
   â”‚  â”œâ”€ (:Entity {name: "Parth Kumar", type: "Person"})
   â”‚  â”œâ”€ (:Entity {name: "GraphRAG Corp", type: "Organization"})
   â”‚  â””â”€ (:Entity {name: "Mumbai", type: "Location"})
   â”‚
   â””â”€ Relationships:
      â”œâ”€ (chunk_0)-[:NEXT]->(chunk_1)
      â”œâ”€ (chunk_1)-[:MENTIONS]->(Parth Kumar)
      â”œâ”€ (Parth Kumar)-[:WORKS_AT]->(GraphRAG Corp)
      â””â”€ (GraphRAG Corp)-[:LOCATED_IN]->(Mumbai)
   â†“
USER QUERY: "Who is the CTO?"
   â†“
VECTOR SEARCH:
   â”œâ”€ Query embedding: [0.111, 0.222, ...]
   â”œâ”€ Find 15 closest chunks
   â””â”€ Top results: Chunks about leadership team
   â†“
ENTITY MATCHING:
   â”œâ”€ Find entities in top chunks: "Parth Kumar", "CTO", "Chief Technology Officer"
   â”œâ”€ Query Neo4j for entity connections
   â””â”€ Get relationships: Parth Kumar â†’ CTO role
   â†“
CONTEXT ASSEMBLY:
   â”œâ”€ Chunk text: "...Parth Kumar: Chief Technology Officer..."
   â”œâ”€ Entity info: Parth Kumar is type "Person", role "CTO"
   â”œâ”€ Relationships: Works for GraphRAG Corp
   â””â”€ Citations: From "company_info" source
   â†“
LLM GENERATION:
   â”œâ”€ Prompt: "Based on these memories, answer: Who is the CTO?"
   â”œâ”€ Context: [Memory 1, Memory 2, Memory 8]
   â””â”€ Rules: [Citations required, fact-only, no inference]
   â†“
ANSWER: "The CTO of GraphRAG Corp is Parth Kumar, as stated in the 
         company information document (Source: company_info)."

"""

# ============================================================================
# IMPLEMENTATION DETAILS
# ============================================================================

"""
FILE LOCATIONS
==============

pdf_processor.py
â”œâ”€ PDFProcessor class
â”‚  â”œâ”€ extract_pdf_text(pdf_path)      â†’ PyPDF2 extraction
â”‚  â”œâ”€ extract_txt_text(txt_path)      â†’ file.read()
â”‚  â”œâ”€ extract_docx_text(docx_path)    â†’ python-docx extraction
â”‚  â”œâ”€ process_file(file_path)         â†’ Main upload function
â”‚  â”œâ”€ process_directory(dir_path)     â†’ Batch upload
â”‚  â”œâ”€ upload_from_url(url)            â†’ Download + process
â”‚  â””â”€ _clean_text(text)               â†’ Text cleaning
â”‚
â””â”€ Convenience functions:
   â”œâ”€ upload_pdf(file_path, source)   â†’ Single file upload
   â”œâ”€ upload_directory(dir_path)      â†’ Directory upload
   â””â”€ upload_from_url(url)            â†’ URL download

ingest.py
â”œâ”€ DocumentIngestion.add_document()   â†’ Adds to memory
â”œâ”€ split_document()                  â†’ Chunking (memory_manager.py)
â”œâ”€ add_chunk_memory()                â†’ FAISS storage (memory_manager.py)
â””â”€ relate_chunks()                   â†’ Create relationships (memory_manager.py)

memory_manager.py
â”œâ”€ split_document()
â”‚  â”œâ”€ Input: Raw text from PDF
â”‚  â”œâ”€ Logic: Sentence-based splitting at (.!?)
â”‚  â”œâ”€ Features:
â”‚  â”‚  â”œâ”€ Preserve entity relationships
â”‚  â”‚  â”œâ”€ Detect negation keywords [NEG]
â”‚  â”‚  â”œâ”€ Maintain 80-100 words per chunk
â”‚  â”‚  â””â”€ 15-word overlap between chunks
â”‚  â””â”€ Output: List of chunks
â”‚
â”œâ”€ add_chunk_memory(chunk, source)
â”‚  â”œâ”€ Create OpenAI embedding
â”‚  â”œâ”€ Store in FAISS
â”‚  â”œâ”€ Store in faiss_index/
â”‚  â””â”€ Set priority (default: 1.0)
â”‚
â””â”€ retrieve_relevant_memories(query)
   â”œâ”€ Create query embedding
   â”œâ”€ Search FAISS (k=15)
   â”œâ”€ Re-rank by entities
   â””â”€ Return top matches

retrieve.py
â”œâ”€ ask_question(query)
â”‚  â”œâ”€ Call retrieve_relevant_memories()
â”‚  â”œâ”€ Format context
â”‚  â”œâ”€ Call LLM with prompt rules
â”‚  â””â”€ Print answer with citations
â”‚
â””â”€ generate_answer(memories, query)
   â”œâ”€ Apply 6 LLM rules
   â”œâ”€ Use GPT-4o-mini model
   â””â”€ Return structured answer

FAISS STORAGE
=============
faiss_index/
â”œâ”€ index.bin               (Vector index - 500MB for 100k chunks)
â”œâ”€ metadata.json          (Chunk information)
â””â”€ chunk_store.pkl        (Actual chunk texts)

NEO4J DATABASE
==============
bolt://localhost:7687
â”œâ”€ Nodes: Chunks, Entities (Person, Organization, Location)
â””â”€ Relationships: NEXT, MENTIONS, WORKS_AT, LOCATED_IN, etc.

"""

# ============================================================================
# USAGE EXAMPLES
# ============================================================================

"""
EXAMPLE 1: Upload single PDF file
==================================

python main.py
â†’ Select: [2] Upload PDF/Document file
â†’ Select: [1] Upload single file
â†’ Enter path: /path/to/document.pdf
â†’ Enter source (optional): my_document

RESULT:
âœ… File uploaded successfully!
   - Characters extracted: 5000
   - Chunks created: 50
   - Stored in FAISS
   - Entities indexed in Neo4j

---

EXAMPLE 2: Upload entire directory
===================================

python main.py
â†’ Select: [2] Upload PDF/Document file
â†’ Select: [2] Upload entire directory
â†’ Enter directory: /path/to/documents/
â†’ Enter source prefix: quarterly_reports

RESULT:
âœ… Uploaded 12 files
   - report_q1_2025.pdf
   - report_q2_2025.pdf
   - report_q3_2025.pdf
   - ...

---

EXAMPLE 3: Upload from Python script
=====================================

from pdf_processor import upload_pdf, upload_directory
from memory_manager import load_vector_store

load_vector_store()

# Single file
result = upload_pdf("/path/to/document.pdf", source="my_doc")

# Directory
results = upload_directory("/path/to/docs/", source_prefix="quarterly")

# Access results
print(f"Extracted characters: {result['extracted_chars']}")
print(f"Chunks created: {result['doc_info']['chunk_count']}")
print(f"Source: {result['source']}")

---

EXAMPLE 4: Query uploaded documents
====================================

python main.py
â†’ [1] Add text document / [2] Upload file / [3] Ask question...
â†’ Select: [3] Ask a question
â†’ Enter: "What is mentioned in the uploaded document about X?"

SYSTEM FLOW:
1. Convert question to embedding
2. Search uploaded document chunks in FAISS
3. Re-rank by entity matching
4. Query Neo4j for relationships
5. Generate answer with citations from source
6. Return: "Based on [Source Name], ..."

"""

# ============================================================================
# KEY FEATURES & BENEFITS
# ============================================================================

"""
FEATURES
========

âœ… Multiple Format Support
   â””â”€ PDF, TXT, DOCX, PPTX

âœ… Batch Upload
   â””â”€ Process entire directories at once

âœ… Text Extraction
   â”œâ”€ Page-aware extraction (PDFs)
   â”œâ”€ UTF-8 text handling
   â””â”€ Structure preservation

âœ… Smart Chunking
   â”œâ”€ Sentence-based (not word-based)
   â”œâ”€ Negation detection [NEG] markers
   â”œâ”€ Entity relationship preservation
   â””â”€ 80-100 words per chunk

âœ… Semantic Indexing
   â”œâ”€ OpenAI embeddings
   â”œâ”€ FAISS vector storage
   â””â”€ Neo4j entity relationships

âœ… Hybrid Search
   â”œâ”€ Vector similarity (FAISS)
   â”œâ”€ Entity matching (Neo4j)
   â””â”€ Re-ranking for accuracy

âœ… Citation Tracking
   â”œâ”€ Source attribution
   â”œâ”€ Timestamp recording
   â”œâ”€ Chunk tracing
   â””â”€ Easy verification

BENEFITS
========

ðŸ“ˆ Accuracy
   â””â”€ 100% on uploaded document queries

ðŸš€ Speed
   â”œâ”€ FAISS: O(1) lookup for 15 chunks
   â”œâ”€ Neo4j: Instant relationship queries
   â””â”€ Total: <1 second response time

ðŸ’¾ Scalability
   â”œâ”€ FAISS: Handles 100k+ chunks
   â”œâ”€ Neo4j: Unlimited entity relationships
   â””â”€ Multi-document support

ðŸ” Discoverability
   â”œâ”€ Find connections across documents
   â”œâ”€ Relationship traversal
   â””â”€ Entity linking

ðŸ“ Traceability
   â”œâ”€ All answers cite sources
   â”œâ”€ Timestamp all uploads
   â””â”€ Track chunk lineage

ðŸ› ï¸ Flexibility
   â”œâ”€ Add new documents anytime
   â”œâ”€ Batch or individual upload
   â”œâ”€ Multiple source tracking
   â””â”€ Query history preservation

"""

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

"""
ERROR: File not found
SOLUTION: Provide absolute path: /home/user/documents/file.pdf

ERROR: Unsupported format
SOLUTION: Convert to PDF/TXT/DOCX using online tools or:
          - PDF: Already supported
          - Word: Use .docx format
          - Images: Use OCR first (e.g., pytesseract)

ERROR: PDF text extraction empty
SOLUTION: Some PDFs have scanned images. Use OCR:
          pip install pytesseract pillow
          
ERROR: Large file hangs
SOLUTION: System will chunk automatically. For 1GB+ files:
          1. Split file into 50MB chunks manually
          2. Upload chunks separately
          3. System will link them via Neo4j

ERROR: FAISS out of memory
SOLUTION: Increase chunk size from 80 to 150 words
          Or: Split documents and upload in batches

ERROR: Neo4j connection failed
SOLUTION: Ensure Neo4j is running:
          docker start neo4j  (if using Docker)
          systemctl start neo4j  (if installed locally)

"""

# ============================================================================
# ADVANCED USAGE
# ============================================================================

"""
SCENARIO 1: Corporate Document Management
===========================================

Upload structure:
â”œâ”€ /contracts/
â”‚  â”œâ”€ agreement_2024.pdf
â”‚  â”œâ”€ agreement_2025.pdf
â”‚  â””â”€ nda.pdf
â”œâ”€ /reports/
â”‚  â”œâ”€ quarterly_q1.pdf
â”‚  â”œâ”€ quarterly_q2.pdf
â”‚  â””â”€ annual_2024.pdf
â””â”€ /policies/
   â”œâ”€ employee_handbook.pdf
   â”œâ”€ it_security.pdf
   â””â”€ code_of_conduct.pdf

Upload command:
python -c "
from pdf_processor import upload_directory
from memory_manager import load_vector_store

load_vector_store()
upload_directory('/contracts/', source_prefix='contract')
upload_directory('/reports/', source_prefix='report')
upload_directory('/policies/', source_prefix='policy')
"

Queries:
- "What is the NDA about?"
- "Find all contracts mentioning Company X"
- "What are the employee policies?"
- "Compare Q1 and Q2 reports"

Result: All answers cite source document and can be traced

---

SCENARIO 2: Research Paper Analysis
====================================

Upload: 50 research papers on AI/ML

Query: "What does paper X say about transformer models?"

System:
1. Searches all 50 papers
2. Finds papers mentioning "transformer"
3. Retrieves relevant chunks
4. Returns answer with paper citations

Benefits:
- Skip manually reading 50 papers
- Find connections across papers
- Track sources for literature review

---

SCENARIO 3: Real-time News Ingestion
====================================

from pdf_processor import upload_from_url

# Download and upload news articles
upload_from_url(
    'https://news.example.com/article.pdf',
    filename='news_2024_12_19.pdf'
)

Query: "What news about AI was published today?"

System: Immediately searchable

"""

print(__doc__)
