â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      OPENAI API USAGE IN GRAPHRAG                          â•‘
â•‘                                                                            â•‘
â•‘                Where and How OpenAI APIs are Used                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. OVERVIEW - TWO MAIN USES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Your GraphRAG system uses OpenAI API in TWO main places:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. EMBEDDINGS API (memory_manager.py)                           â”‚
â”‚    â””â”€ For: Converting text to semantic vectors (1536-dim)       â”‚
â”‚    â””â”€ Model: text-embedding-3-small                            â”‚
â”‚    â””â”€ Purpose: Store in FAISS for similarity search            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. CHAT API (retrieve.py)                                       â”‚
â”‚    â””â”€ For: Generating answers from context                      â”‚
â”‚    â””â”€ Model: gpt-4o-mini                                        â”‚
â”‚    â””â”€ Purpose: Answer user questions with reasoning             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
2. EMBEDDINGS API - DETAILED USAGE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FILE: memory_manager.py
LINES: 1-25 (Initialization), 140-160 (Usage), 340-360 (Batch usage)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INITIALIZATION (Lines 1-25):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from langchain_openai import OpenAIEmbeddings
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Create embeddings instance
api_key = os.getenv("OPENAI_API_KEY")
embeddings = OpenAIEmbeddings(api_key=api_key)

What happens:
â”œâ”€ Import OpenAIEmbeddings from LangChain
â”œâ”€ Read OPENAI_API_KEY from environment (.env file)
â”œâ”€ Create global embeddings object
â””â”€ Ready to convert text to vectors

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USE CASE 1: Adding Single Document (Lines 140-160)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Location: Function add_chunk_memory(chunk, priority, source)

def add_chunk_memory(chunk, priority=1.0, source="general"):
    """Add a chunk to memory with embedding"""
    global vector_store
    
    try:
        # FAISS converts chunk to vector using OpenAI API
        if vector_store is None:
            vector_store = FAISS.from_texts(
                [chunk],                    # Text to embed
                embedding=embeddings        # Uses OpenAI API here
            )
        else:
            vector_store.add_texts(
                [chunk],
                metadatas=[{"source": source, "timestamp": datetime.now().isoformat()}]
            )
        save_vector_store()
        
    except Exception as e:
        print(f"[ERROR] {e}")

What happens:
â”œâ”€ Input: chunk = "Parth Kumar is the CTO"
â”œâ”€ OpenAI API Call: text-embedding-3-small model
â”œâ”€ Output: Vector [0.123, 0.456, ..., 1536 dimensions]
â”œâ”€ Storage: Saved in FAISS index
â””â”€ Cost: ~$0.02 per 1M tokens

Example Flow:
    Chunk: "Parth is a Software Engineer at DRC Systems"
         â†“
    OpenAI Embedding API
         â†“
    Vector: [0.234, -0.567, 0.890, ..., <1536 numbers>]
         â†“
    FAISS Storage (faiss_index/index.bin)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USE CASE 2: Batch Adding Documents (Lines 340-360)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Location: Function add_memory_to_store(texts, metadatas)

def add_memory_to_store(texts, metadatas=None):
    """Add batch of memories to store"""
    global vector_store
    
    if not texts:
        return
    
    try:
        # OpenAI API converts ALL texts to vectors at once
        if vector_store is None:
            vector_store = FAISS.from_texts(
                texts,                      # List of texts
                embedding=embeddings        # OpenAI API called for each
            )
        else:
            vector_store.add_texts(texts, metadatas=metadatas)
        
        save_vector_store()
        print(f"[ADD] {len(texts)} memories added")
        
    except Exception as e:
        print(f"[ERROR] {e}")

What happens:
â”œâ”€ Input: texts = ["Chunk 1", "Chunk 2", "Chunk 3", ...]
â”œâ”€ OpenAI API Calls: One per chunk (batched)
â”œâ”€ Output: Vectors for all chunks
â”œâ”€ Storage: All stored in FAISS
â””â”€ Cost: Proportional to total tokens

Example:
    Texts: ["Parth works at DRC", "Raju is his friend", "Adil is..."]
         â†“
    3x OpenAI Embedding API calls (in parallel)
         â†“
    3 Vectors created
         â†“
    All stored in FAISS

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USE CASE 3: Query Embedding (Memory_manager.py, Line 165)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Location: Function retrieve_relevant_memories(query)

def retrieve_relevant_memories(query, k=15):
    """Retrieve relevant memories for a query"""
    
    if vector_store is None:
        return []
    
    try:
        # Convert query to vector using OpenAI API
        results = vector_store.similarity_search(
            query,                  # User's question
            k=k                     # Return top 15
        )
        
        # OpenAI Embedding API called here for query
        # Returns documents similar to query vector

What happens:
â”œâ”€ Input: query = "Who is the CTO?"
â”œâ”€ OpenAI API Call: Convert question to vector
â”œâ”€ Vector: [0.111, 0.222, ..., 1536 dims]
â”œâ”€ FAISS Search: Find 15 closest chunk vectors
â”œâ”€ Output: Top 15 matching chunks
â””â”€ Cost: ~$0.02 per 1M tokens

Example:
    Q: "Who is the CTO?"
         â†“
    OpenAI Embedding: [0.111, 0.222, ...]
         â†“
    FAISS Similarity Search (fast, local)
         â†“
    Returns: Top 15 chunks mentioning "CTO"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EMBEDDINGS API SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model:              text-embedding-3-small
Input:              Text (any length)
Output:             1536-dimensional vector
Cost:               $0.02 per 1M tokens (very cheap)
Speed:              ~50-100ms per request
Usage in GraphRAG:  3 main uses:
                    â”œâ”€ add_chunk_memory() - Add one chunk
                    â”œâ”€ add_memory_to_store() - Add batch
                    â””â”€ retrieve_relevant_memories() - Query search

Total Calls Per Upload:
    200KB PDF â†’ 50 chunks â†’ ~50 OpenAI API calls
    
Total Calls Per Query:
    1 query + 15 chunk re-embeddings â†’ ~2 OpenAI API calls


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
3. CHAT API - DETAILED USAGE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FILE: retrieve.py
LINES: 1-15 (Initialization), 30-90 (Usage)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INITIALIZATION (Lines 1-15):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize LLM
llm = ChatOpenAI(
    model="gpt-4o-mini",
    api_key=os.getenv("OPENAI_API_KEY")
)

What happens:
â”œâ”€ Import ChatOpenAI from LangChain
â”œâ”€ Create LLM instance for gpt-4o-mini
â”œâ”€ Ready to generate answers
â””â”€ Authentication set via environment variable

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USE CASE: Generate Answer (Lines 30-90)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Location: Function generate_answer(query, memories)

def generate_answer(self, query, memories):
    """Generate an answer using LLM"""
    
    # Format memories with context
    relevant_context = ""
    for idx, m in enumerate(memories, 1):
        relevant_context += f"\n[Memory {idx}]: {extract_text(m)}"
    
    # Create explicit prompt with 6 rules
    prompt = f"""You are a helpful assistant answering questions based on personal memories.
    
CRITICAL INSTRUCTIONS:

1. **YES/NO QUESTIONS**: Answer directly with YES or NO first
2. **NEGATION HANDLING**: Pay attention to [NEG] markers and NOT keywords
3. **MULTIPLE ITEMS**: List ALL items mentioned, not just one
4. **PERSPECTIVE CLARITY**: I = Parth
5. **FACTS ONLY**: Don't infer or assume
6. **MEMORY CITATION**: Cite which memory supports answer

RELEVANT MEMORIES:
{relevant_context}

User Question: {query}
"""

    # OpenAI Chat API called here
    response = llm.invoke(prompt)
    answer = response.content
    
    return answer

What happens:
â”œâ”€ Input: query + 15 relevant memories
â”œâ”€ Format: Detailed prompt with 6 explicit rules
â”œâ”€ OpenAI Chat API Call: gpt-4o-mini model
â”œâ”€ Output: Reasoned answer with citations
â””â”€ Cost: ~$0.15 per 1M input tokens, $0.60 per 1M output tokens

Example:
    Prompt + Context (15 chunks + explicit rules)
         â†“
    OpenAI gpt-4o-mini Chat API
         â†“
    Analysis of memories
         â†“
    Generate answer following 6 rules
         â†“
    Output: "Yes, Parth Kumar is the CTO. Memory 1 mentions..."

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CHAT API SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model:              gpt-4o-mini (fast, cheap version of GPT-4)
Input:              Question + 15 memory chunks + system prompt
Output:             Natural language answer with citations
Cost:               $0.15 per 1M input tokens
                    $0.60 per 1M output tokens
Speed:              1-2 seconds per query
Usage in GraphRAG:  1 main use:
                    â””â”€ generate_answer() - Answer questions

Tokens per Query:
    15 memories (avg 200 tokens each) = 3000 tokens input
    Answer (avg 100 tokens) = 100 tokens output
    Total: 3100 tokens per query
    Cost: ~$0.00047 per query (half a cent)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
4. OPENAI API KEY SETUP
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

WHERE API KEY COMES FROM:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Environment Variable: OPENAI_API_KEY
   â”œâ”€ Loaded from .env file
   â”œâ”€ Keep it secure (don't commit to Git)
   â””â”€ Create account: https://platform.openai.com

2. In memory_manager.py (Line 22):
   api_key = os.getenv("OPENAI_API_KEY")

3. In retrieve.py (Line 15):
   api_key=os.getenv("OPENAI_API_KEY")

4. .env file should contain:
   OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx

HOW TO GET YOUR KEY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Visit: https://platform.openai.com/account/api-keys
2. Create new secret key
3. Copy it
4. Create .env file in project root:
   OPENAI_API_KEY=sk-proj-...
5. Save (don't share!)

VERIFY IT WORKS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
python -c "
from dotenv import load_dotenv
import os
load_dotenv()
key = os.getenv('OPENAI_API_KEY')
print('âœ… API Key loaded' if key else 'âŒ API Key not found')
print('Key starts with:', key[:10] + '...' if key else 'N/A')
"


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
5. CODE FLOW - WHERE OPENAI IS CALLED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

UPLOAD FLOW:
â•â•â•â•â•â•â•â•â•â•â•â•

User runs: python main.py â†’ [2] Upload PDF

[ingest.py]
    â†“
ingestion.add_document(text)
    â†“
[memory_manager.py]
    â†“
split_document(text) â† No API call
    â†“
for chunk in chunks:
    add_chunk_memory(chunk)
        â†“
        FAISS.from_texts(chunk, embedding=embeddings)
            â†“
            ğŸ”´ OPENAI API CALL #1 (Embedding API)
                Model: text-embedding-3-small
                Input: chunk text
                Output: vector [1536 dimensions]
        â†“
        Save vector to FAISS
        â†“
Save FAISS to disk â† No API call


QUERY FLOW:
â•â•â•â•â•â•â•â•â•â•â•

User runs: python main.py â†’ [3] Ask question

[retrieve.py]
    â†“
ask_question(query)
    â†“
retrieve_relevant_memories(query)
    â†“
vector_store.similarity_search(query, k=15)
    â†“
    ğŸ”´ OPENAI API CALL #1 (Embedding API)
        Model: text-embedding-3-small
        Input: query text
        Output: vector [1536 dimensions]
    â†“
    FAISS local search â† No API call (fast, local)
    â†“
    Returns top 15 chunks
    â†“
generate_answer(query, memories)
    â†“
llm.invoke(prompt_with_memories)
    â†“
    ğŸ”´ OPENAI API CALL #2 (Chat API)
        Model: gpt-4o-mini
        Input: prompt + 15 memories
        Output: generated answer
    â†“
Return answer to user


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
6. COST ANALYSIS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EMBEDDING API COSTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model: text-embedding-3-small
Price: $0.02 per 1M tokens

Per Upload (200KB PDF â†’ 50 chunks):
â”œâ”€ 50 chunks Ã— 100 tokens avg = 5000 tokens
â”œâ”€ Cost: $0.02 Ã— (5000/1M) = $0.0001
â””â”€ Result: Less than 0.01 cents per document

Per Query (1 query + 15 chunks):
â”œâ”€ 1 query (20 tokens) + 15 chunks (3000 tokens) = 3020 tokens
â”œâ”€ Cost: $0.02 Ã— (3020/1M) = $0.00006
â””â”€ Result: Less than 0.01 cents per query


CHAT API COSTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model: gpt-4o-mini
Input:  $0.15 per 1M tokens
Output: $0.60 per 1M tokens

Per Query:
â”œâ”€ Input tokens: 3000 (15 memories + prompt)
â”œâ”€ Output tokens: 150 (answer)
â”œâ”€ Input cost: $0.15 Ã— (3000/1M) = $0.00045
â”œâ”€ Output cost: $0.60 Ã— (150/1M) = $0.00009
â”œâ”€ Total: $0.00054
â””â”€ Result: About 0.05 cents per query


TYPICAL MONTHLY COSTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Scenario: 100 documents + 1000 queries

Upload phase (100 docs):
â”œâ”€ 100 Ã— $0.0001 = $0.01
â””â”€ Cost: Less than 1 cent

Query phase (1000 queries):
â”œâ”€ Embedding: 1000 Ã— $0.00006 = $0.06
â”œâ”€ Chat: 1000 Ã— $0.00045 = $0.45
â”œâ”€ Total: $0.51
â””â”€ Cost: About 50 cents

Total Monthly: ~$0.52 (half a cent)
Very Cheap! âœ…


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
7. API CALLS BREAKDOWN
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

WHAT EACH API DOES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. EMBEDDING API (text-embedding-3-small)
   â”œâ”€ Input: Any text (sentence, paragraph, document)
   â”œâ”€ Output: Vector with 1536 numbers
   â”œâ”€ Purpose: Convert text to semantic representation
   â”œâ”€ Used for: FAISS storage & similarity search
   â”œâ”€ Speed: Fast (50-100ms)
   â”œâ”€ Cost: Very cheap ($0.02/1M tokens)
   â””â”€ Example:
       Input: "Parth is the CTO"
       Output: [0.234, -0.567, 0.890, ..., <1536 numbers>]

2. CHAT API (gpt-4o-mini)
   â”œâ”€ Input: Prompt with instructions + context
   â”œâ”€ Output: Natural language response
   â”œâ”€ Purpose: Reason about memories and answer questions
   â”œâ”€ Used for: Generate answers with citations
   â”œâ”€ Speed: Fast (1-2 seconds)
   â”œâ”€ Cost: Cheap ($0.15/$0.60 per 1M tokens)
   â””â”€ Example:
       Input: "Based on memories, who is the CTO?"
       Output: "Parth Kumar is the CTO (Memory 1)"


HOW THEY WORK TOGETHER:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Upload PDF
    PDF â†’ Extract text â†’ Create chunks
    â†“
    For each chunk:
    â””â”€ OpenAI Embedding API â†’ Convert to vector
    
    Stored in FAISS (no API needed for retrieval, all local)

Step 2: Ask Question
    Question â†’ OpenAI Embedding API â†’ Convert to vector
    â†“
    FAISS local search (NO API) â†’ Find 15 similar chunks
    â†“
    Chunks + Question â†’ OpenAI Chat API â†’ Generate answer
    
    Return answer with citations


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
8. LIBRARIES USED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

LANGCHAIN (Wrapper around OpenAI):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain_openai import OpenAIEmbeddings
â””â”€ Easy wrapper for OpenAI embeddings
â””â”€ Automatically handles API calls
â””â”€ Integrates with FAISS

from langchain_openai import ChatOpenAI
â””â”€ Easy wrapper for OpenAI chat
â””â”€ Simplified API for answering questions
â””â”€ Built-in prompt handling

from langchain_community.vectorstores import FAISS
â””â”€ Vector database integration
â””â”€ Stores OpenAI embeddings
â””â”€ Fast similarity search (local, no API)


WHY NOT USE OPENAI CLIENT DIRECTLY?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You COULD do:
from openai import OpenAI
client = OpenAI(api_key="...")
response = client.embeddings.create(model="text-embedding-3-small", input=text)

But LangChain is better because:
â”œâ”€ Simpler API (less code)
â”œâ”€ Built-in integration with FAISS
â”œâ”€ Better error handling
â”œâ”€ Easier to swap models later
â””â”€ Standard in RAG systems


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
9. ACTUAL CODE EXAMPLES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXAMPLE 1: Add Document with OpenAI Embedding
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# File: memory_manager.py, Line 140

def add_chunk_memory(chunk, priority=1.0, source="general"):
    global vector_store
    try:
        if vector_store is None:
            # ğŸ”´ OpenAI Embedding API called here
            vector_store = FAISS.from_texts(
                [chunk],                    # Text input
                embedding=embeddings        # Uses OpenAI
            )
        else:
            # Add to existing FAISS store
            vector_store.add_texts([chunk])
        
        save_vector_store()
        print(f"[ADD] Chunk stored: '{chunk[:50]}...' with priority {priority}")
    except Exception as e:
        print(f"[ERROR] {e}")

How to trace the OpenAI call:
â”œâ”€ Input: chunk = "Parth Kumar works at DRC Systems"
â”œâ”€ FAISS.from_texts() is called
â”œâ”€ LangChain calls: embeddings.embed_documents([chunk])
â”œâ”€ Which calls: OpenAI API (text-embedding-3-small)
â”œâ”€ Returns: Vector [0.123, -0.456, ..., 1536 dimensions]
â”œâ”€ FAISS stores: Vector + metadata
â””â”€ Cost: $0.02 Ã— (token_count/1M)


EXAMPLE 2: Search with OpenAI Embedding
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# File: memory_manager.py, Line 165

def retrieve_relevant_memories(query, k=15):
    if vector_store is None:
        return []
    
    try:
        # ğŸ”´ OpenAI Embedding API called here
        results = vector_store.similarity_search(query, k=k)
        
        # Returns top 15 chunks most similar to query
        return [r.page_content for r in results]
    except Exception as e:
        return []

How to trace the OpenAI call:
â”œâ”€ Input: query = "Who is the CTO?"
â”œâ”€ vector_store.similarity_search(query) is called
â”œâ”€ LangChain calls: embeddings.embed_query(query)
â”œâ”€ Which calls: OpenAI API (text-embedding-3-small)
â”œâ”€ Returns: Query vector [0.111, 0.222, ...]
â”œâ”€ FAISS search: Finds 15 closest vectors (LOCAL, no API)
â”œâ”€ Returns: Top 15 chunks
â””â”€ Cost: $0.02 Ã— (query_tokens/1M) - very cheap


EXAMPLE 3: Generate Answer with OpenAI Chat
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# File: retrieve.py, Line 75

def generate_answer(self, query, memories):
    # Format memories
    relevant_context = ""
    for idx, m in enumerate(memories, 1):
        relevant_context += f"\n[Memory {idx}]: {m}"
    
    # Create detailed prompt with 6 rules
    prompt = f"""You are answering based on memories.
    
RULES:
1. YES/NO questions: Answer directly
2. [NEG] markers indicate negation
3. List ALL multiple items
4. I = Parth
5. Facts only
6. Cite memory sources

MEMORIES:
{relevant_context}

Question: {query}"""
    
    # ğŸ”´ OpenAI Chat API called here
    response = llm.invoke(prompt)
    
    return response.content

How to trace the OpenAI call:
â”œâ”€ Input: prompt with 15 memories + explicit rules
â”œâ”€ llm.invoke(prompt) is called
â”œâ”€ ChatOpenAI calls: OpenAI API (gpt-4o-mini)
â”œâ”€ Returns: Generated answer following rules
â”œâ”€ Example output: "Yes, Parth Kumar is the CTO (Memory 1)"
â””â”€ Cost: Input $0.15/1M + Output $0.60/1M


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
10. DEBUGGING - HOW TO VERIFY API CALLS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CHECK IF API KEY IS LOADED:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

python -c "
import os
from dotenv import load_dotenv
load_dotenv()
key = os.getenv('OPENAI_API_KEY')
if key:
    print('âœ… API Key found')
    print('First 10 chars:', key[:10] + '...')
else:
    print('âŒ API Key not found')
    print('Check .env file exists with OPENAI_API_KEY=...')
"


TEST EMBEDDING API:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

python -c "
from langchain_openai import OpenAIEmbeddings
import os
from dotenv import load_dotenv

load_dotenv()
embeddings = OpenAIEmbeddings()

# Call API
vector = embeddings.embed_query('Hello world')
print('âœ… Embedding API works')
print('Vector length:', len(vector))
print('First 5 values:', vector[:5])
"

Expected output:
    âœ… Embedding API works
    Vector length: 1536
    First 5 values: [0.123, -0.456, 0.789, ...]


TEST CHAT API:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•

python -c "
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()
llm = ChatOpenAI(model='gpt-4o-mini')

# Call API
response = llm.invoke('What is 2+2?')
print('âœ… Chat API works')
print('Response:', response.content)
"

Expected output:
    âœ… Chat API works
    Response: 2 + 2 = 4


VIEW API USAGE IN OPENAI DASHBOARD:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Go to: https://platform.openai.com/account/usage/overview
2. View:
   â”œâ”€ Total usage this month
   â”œâ”€ Cost breakdown by model
   â”œâ”€ Requests per model
   â””â”€ Cost predictions

You should see:
â”œâ”€ text-embedding-3-small: ~50+ calls per document upload
â”œâ”€ gpt-4o-mini: ~1 call per query
â””â”€ Total cost: Usually <$1/month


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
SUMMARY TABLE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

API              Model                      Where Used              Cost/1M
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMBEDDING        text-embedding-3-small     memory_manager.py       $0.02
CHAT             gpt-4o-mini                retrieve.py             $0.15 in
                                                                    $0.60 out

Usage per Upload (200KB PDF):
    â”œâ”€ 50 embeddings API calls (~$0.0001)
    â”œâ”€ FAISS storage (local, free)
    â””â”€ Total: <$0.01

Usage per Query:
    â”œâ”€ 1 embedding API call (~$0.00006)
    â”œâ”€ 1 chat API call (~$0.00045)
    â””â”€ Total: ~$0.0005 per query

Monthly (100 docs + 1000 queries):
    â”œâ”€ Uploads: $0.01
    â”œâ”€ Queries: $0.51
    â””â”€ Total: ~$0.52/month (very cheap!)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
